import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from pandas import DataFrame
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from sklearn.model_selection import train_test_split
from keras.utils.np_utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.layers import Dropout
import re
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk import word_tokenize
STOPWORDS = set(stopwords.words('portuguese'))
import plotly.graph_objs as go
import plotly.plotly as py
from IPython.core.interactiveshell import InteractiveShell
import plotly.figure_factory as ff
InteractiveShell.ast_node_interactivity = 'all'
from plotly.offline import iplot
import pandas as pd

vle = pd.read_csv("../input/vle.csv",delimiter=',',encoding='latin-1')
vle.head()
vle.drop(['id_site','code_module','code_presentation','week_from', 'week_to'],axis=1,inplace=True)
vle.info()

sns.countplot(vle.activity_type)
plt.xlabel('Label')
plt.title('Distribuição das intenções ')

#Número máximo de palavras mais frequentes.
MAX_NB_WORDS = 3000
# Número máximo de palavras em cada vetor.
MAX_SEQUENCE_LENGTH = 100
EMBEDDING_DIM = 100

#Pré-processamento do texto.
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(vle['activity_type'].values)
word_index = tokenizer.word_index
print('Encontrados %s tokens únicos.' % len(word_index))


X = tokenizer.texts_to_sequences(vle['activity_type'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

Y = pd.get_dummies(vle['activity_type']).values
print('Shape of label tensor:', Y.shape)


#Treinamento
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = None)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

#RNN

#Modelo
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(20, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())


#Execução treinamento
epochs = 2000
batch_size = 64

history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=1,verbose=1)])

accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))
